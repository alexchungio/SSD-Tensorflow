# Copyright 2016 Paul Balanca. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Definition of 300 VGG-based SSD network.

This model was initially introduced in:
SSD: Single Shot MultiBox Detector
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
Cheng-Yang Fu, Alexander C. Berg
https://arxiv.org/abs/1512.02325

Two variants of the model are defined: the 300x300 and 512x512 models, the
latter obtaining a slightly better accuracy on Pascal VOC.

Usage:
    with slim.arg_scope(ssd_vgg.ssd_vgg()):
        outputs, end_points = ssd_vgg.ssd_vgg(inputs)

This network port of the original Caffe model. The padding in TF and Caffe
is slightly different, and can lead to severe accuracy drop if not taken care
in a correct way!

In Caffe, the output size of convolution and pooling layers are computing as
following: h_o = (h_i + 2 * pad_h - kernel_h) / stride_h + 1

Nevertheless, there is a subtle difference between both for stride > 1. In
the case of convolution:
    top_size = floor((bottom_size + 2*pad - kernel_size) / stride) + 1
whereas for pooling:
    top_size = ceil((bottom_size + 2*pad - kernel_size) / stride) + 1
Hence implicitely allowing some additional padding even if pad = 0. This
behaviour explains why pooling with stride and kernel of size 2 are behaving
the same way in TensorFlow and Caffe.

Nevertheless, this is not the case anymore for other kernel sizes, hence
motivating the use of special padding layer for controlling these side-effects.

@@ssd_vgg_300
"""
import math
from collections import namedtuple

import numpy as np
import tensorflow as tf

from libs.config import cfgs
from libs.box_utils import bboxes_utils
from libs.box_utils import box_select
from libs.box_utils import encode_decode
from libs.nets import custom_layers
from libs.box_utils import anchor_utils

from libs.losses import losses
import os


slim = tf.contrib.slim

# =========================================================================== #
# SSD class definition.
# =========================================================================== #
SSDParams = namedtuple('SSDParameters', ['img_shape',
                                         'num_classes',
                                         'no_annotation_label',
                                         'feat_layers',
                                         'feat_shapes',
                                         'anchor_size_bounds',
                                         'anchor_sizes',
                                         'anchor_ratios',
                                         'anchor_steps',
                                         'anchor_offset',
                                         'normalizations',
                                         'prior_scaling'
                                         ])


class SSDNet(object):
    """Implementation of the SSD VGG-based 300 network.

    The default features layers with 300x300 image input are:
      conv4 ==> 38 x 38
      conv7 ==> 19 x 19
      conv8 ==> 10 x 10
      conv9 ==> 5 x 5
      conv10 ==> 3 x 3
      conv11 ==> 1 x 1
    The default image size used to train this network is 300x300.
    """
    default_params = SSDParams(
        img_shape=(300, 300),
        num_classes=21,
        no_annotation_label=21,
        feat_layers=['block4', 'block7', 'block8', 'block9', 'block10', 'block11'],
        feat_shapes=[(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)],
        anchor_size_bounds=[0.15, 0.90],
        # anchor_size_bounds=[0.20, 0.90],
        anchor_sizes=[(21., 45.),
                      (45., 99.),
                      (99., 153.),
                      (153., 207.),
                      (207., 261.),
                      (261., 315.)],
        # anchor_sizes=[(30., 60.),
        #               (60., 111.),
        #               (111., 162.),
        #               (162., 213.),
        #               (213., 264.),
        #               (264., 315.)],
        anchor_ratios=[[2, .5],
                       [2, .5, 3, 1./3],
                       [2, .5, 3, 1./3],
                       [2, .5, 3, 1./3],
                       [2, .5],
                       [2, .5]],
        anchor_steps=[8, 16, 32, 64, 100, 300],
        anchor_offset=0.5,
        normalizations=[20, -1, -1, -1, -1, -1],
        prior_scaling=[0.1, 0.1, 0.2, 0.2]
        )

    def __init__(self, params=None):
        """Init the SSD net with some parameters. Use the default ones
        if none provided.
        """
        if isinstance(params, SSDParams):
            self.params = params
        else:
            self.params = SSDNet.default_params
        # if cfgs.DATA_FORMAT == "NHWC":
        #     self.images_batch = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, None, None, 3],
        #                                                  name="input_images")
        # else:
        #     self.images_batch = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 3, None, None],
        #                                                  name="input_images")

        # self.labels_batch = tf.placeholder(dtype=tf.int32, shape=[None, None, cfgs.NUM_CLASS+1], name="gt_labels")
        # self.bboxes_batch = tf.placeholder(dtype=tf.float32, shape=(None, None, None, 4), name="gt_bboxes")
        # self.scores_batch  = tf.placeholder(dtype=tf.float32, shape=(None, None, 1), name="gt_scores")

        self.global_step = tf.train.get_or_create_global_step()

    # ======================================================================= #
    def net(self, inputs,
            is_training=True,
            update_feat_shapes=True,
            dropout_keep_prob=0.5,
            prediction_fn=slim.softmax,
            reuse=None,
            scope='ssd_300_vgg'):
        """SSD network definition.
        """
        r = ssd_net(inputs,
                    num_classes=self.params.num_classes,
                    feat_layers=self.params.feat_layers,
                    anchor_sizes=self.params.anchor_sizes,
                    anchor_ratios=self.params.anchor_ratios,
                    normalizations=self.params.normalizations,
                    is_training=is_training,
                    dropout_keep_prob=dropout_keep_prob,
                    prediction_fn=prediction_fn,
                    reuse=reuse,
                    scope=scope)
        # Update feature shapes (try at least!)
        if update_feat_shapes:
            shapes = ssd_feat_shapes_from_net(r[0], self.params.feat_shapes)
            self.params = self.params._replace(feat_shapes=shapes)
        return r

    def arg_scope(self, weight_decay=0.0005, data_format='NHWC'):
        """Network arg_scope.
        """
        return ssd_arg_scope(weight_decay, data_format=data_format)

    def arg_scope_caffe(self, caffe_scope):
        """Caffe arg_scope used for weights importing.
        """
        return ssd_arg_scope_caffe(caffe_scope)

    # ======================================================================= #
    def update_feature_shapes(self, predictions):
        """Update feature shapes from predictions collection (Tensor or Numpy
        array).
        """
        shapes = ssd_feat_shapes_from_net(predictions, self.params.feat_shapes)
        self.params = self.params._replace(feat_shapes=shapes)

    def make_anchors(self, img_shape, dtype=np.float32):
        """Compute the default anchor boxes, given an image shape.
        """
        return anchor_utils.ssd_anchors_all_layers(img_shape,
                                                  self.params.feat_shapes,
                                                  self.params.anchor_sizes,
                                                  self.params.anchor_ratios,
                                                  self.params.anchor_steps,
                                                  self.params.anchor_offset,
                                                  dtype)


    def bboxes_encode(self, labels, bboxes, anchors,
                      scope=None):
        """Encode labels and bounding boxes.
        """
        return encode_decode.tf_ssd_bboxes_encode(
            labels, bboxes, anchors,
            self.params.num_classes,
            self.params.no_annotation_label,
            ignore_threshold=0.5,
            prior_scaling=self.params.prior_scaling,
            scope=scope)

    def bboxes_decode(self, feat_localizations, anchors,
                      scope='ssd_bboxes_decode'):
        """Encode labels and bounding boxes.
        """
        return encode_decode.tf_ssd_bboxes_decode(
            feat_localizations, anchors,
            prior_scaling=self.params.prior_scaling,
            scope=scope)

    def detected_bboxes(self, predictions, localisations,
                        select_threshold=None, nms_threshold=0.5,
                        clipping_bbox=None, top_k=400, keep_top_k=200):
        """Get the detected bounding boxes from the SSD network output.
        """
        # Select top_k bboxes from predictions, and clip
        rscores, rbboxes = \
            box_select.tf_ssd_bboxes_select(predictions, localisations,
                                            select_threshold=select_threshold,
                                            num_classes=self.params.num_classes)
        rscores, rbboxes = \
            bboxes_utils.bboxes_sort(rscores, rbboxes, top_k=top_k)
        # Apply NMS algorithm.
        rscores, rbboxes = \
            bboxes_utils.bboxes_nms_batch(rscores, rbboxes,
                                 nms_threshold=nms_threshold,
                                 keep_top_k=keep_top_k)
        if clipping_bbox is not None:
            rbboxes = bboxes_utils.bboxes_clip(clipping_bbox, rbboxes)
        return rscores, rbboxes

    def losses(self, logits, localisations,
               gclasses, glocalisations, gscores,
               match_threshold=0.5,
               negative_ratio=3.,
               alpha=1.,
               label_smoothing=0.,
               scope='ssd_losses'):
        """Define the SSD network losses.
        """
        return losses.ssd_losses(logits, localisations,
                          gclasses, glocalisations, gscores,
                          match_threshold=match_threshold,
                          negative_ratio=negative_ratio,
                          alpha=alpha,
                          label_smoothing=label_smoothing,
                          scope=scope)

    def learning_rate(self, global_step, boundaries, rates, warmup=False):
        """Manually stepped learning rate schedule.

        This function provides fine grained control over learning rates.  One must
        specify a sequence of learning rates as well as a set of integer steps
        at which the current learning rate must transition to the next.  For example,
        if boundaries = [5, 10] and rates = [.1, .01, .001], then the learning
        rate returned by this function is .1 for global_step=0,...,4, .01 for
        global_step=5...9, and .001 for global_step=10 and onward.

        Args:
          global_step: int64 (scalar) tensor representing global step.
          boundaries: a list of global steps at which to switch learning
            rates.  This list is assumed to consist of increasing positive integers.
          rates: a list of (float) learning rates corresponding to intervals between
            the boundaries.  The length of this list must be exactly
            len(boundaries) + 1.
          warmup: Whether to linearly interpolate learning rate for steps in
            [0, boundaries[0]].

        Returns:
          If executing eagerly:
            returns a no-arg callable that outputs the (scalar)
            float tensor learning rate given the current value of global_step.
          If in a graph:
            immediately returns a (scalar) float tensor representing learning rate.
        Raises:
          ValueError: if one of the following checks fails:
            1. boundaries is a strictly increasing list of positive integers
            2. len(rates) == len(boundaries) + 1
            3. boundaries[0] != 0
        """
        if any([b < 0 for b in boundaries]) or any(
                [not isinstance(b, int) for b in boundaries]):
            raise ValueError('boundaries must be a list of positive integers')
        if any([bnext <= b for bnext, b in zip(boundaries[1:], boundaries[:-1])]):
            raise ValueError('Entries in boundaries must be strictly increasing.')
        if any([not isinstance(r, float) for r in rates]):
            raise ValueError('Learning rates must be floats')
        if len(rates) != len(boundaries) + 1:
            raise ValueError('Number of provided learning rates must exceed '
                             'number of boundary points by exactly 1.')

        if boundaries and boundaries[0] == 0:
            raise ValueError('First step cannot be zero.')

        if warmup and boundaries:
            slope = (rates[1] - rates[0]) * 1.0 / boundaries[0]
            warmup_steps = list(range(boundaries[0]))
            warmup_rates = [rates[0] + slope * step for step in warmup_steps]
            boundaries = warmup_steps + boundaries
            rates = warmup_rates + rates[1:]
        else:
            boundaries = [0] + boundaries
        num_boundaries = len(boundaries)

        def eager_decay_rate():
            """Callable to compute the learning rate."""
            rate_index = tf.reduce_max(tf.where(
                tf.greater_equal(global_step, boundaries),
                list(range(num_boundaries)),
                [0] * num_boundaries))
            return tf.reduce_sum(rates * tf.one_hot(rate_index, depth=num_boundaries),
                                 name='learning_rate')

        if tf.executing_eagerly():
            return eager_decay_rate
        else:
            return eager_decay_rate()

    def gather_loss(clone, regularization_losses):
        """Gather the loss for a single clone.

        Args:
            clone: A Clone namedtuple.
            regularization_losses: Possibly empty list of regularization_losses
                to add to the clone losses.

        Returns:
            A tensor for the total loss for the clone.  Can be None.
        """
        # The return value.
        sum_loss = None
        # Individual components of the loss that will need summaries.
        regularization_loss = None
        # Compute and aggregate losses on the clone device.
        # with tf.device('/device:CPU:0'):
        all_losses = []
        clone_losses = tf.get_collection(tf.GraphKeys.LOSSES, scope=None)
        if clone_losses:
            clone_loss = tf.add_n(clone_losses, name='clone_loss')

            all_losses.append(clone_loss)
        if regularization_losses:
            regularization_loss = tf.add_n(regularization_losses,
                                           name='regularization_loss')
            all_losses.append(regularization_loss)
        if all_losses:
            sum_loss = tf.add_n(all_losses)
        # tf.summary.scalar(clone.scope + '/clone_loss', clone_loss)
        if regularization_loss is not None:
            tf.summary.scalar('regularization_loss', regularization_loss)
        return sum_loss


    def optimize_gradient(self, optimizer, regularization_losses=None, **kwargs):
        """Compute clone losses and gradients for the given list of `Clones`.

        Note: The regularization_losses are added to the first clone losses.

        Args:
          clones: List of `Clones` created by `create_clones()`.
          optimizer: An `Optimizer` object.
          regularization_losses: Optional list of regularization losses. If None it
             will gather them from tf.GraphKeys.REGULARIZATION_LOSSES. Pass `[]` to
             exclude them.
          **kwargs: Optional list of keyword arguments to pass to `compute_gradients`.

        Returns:
          A tuple (total_loss, grads_and_vars).
            - total_loss: A Tensor containing the average of the clone losses
                including the regularization loss.
            - grads_and_vars: A List of tuples (gradient, variable) containing the
                sum of the gradients for each variable.
        """
        if regularization_losses is None:
            regularization_losses = tf.get_collection(
                tf.GraphKeys.REGULARIZATION_LOSSES)

        total_loss = self.gather_loss(regularization_losses)

        optimizer_gradients = None
        if total_loss is not None:
            optimizer_gradients = optimizer.compute_gradients(total_loss, **kwargs)

        return total_loss, optimizer_gradients

    # def fill_feed_dict(self, image_batch, labels_batch, bboxes_batch, scores_batch):
    #
    #     feed_dict = {self.images_batch: image_batch,
    #                  self.labels_batch: labels_batch,
    #                  self.bboxes_batch: bboxes_batch,
    #                  self.scores_batch: scores_batch
    #                  }
    #     return feed_dict

    def restore_ckpt(self, sess):
        """
        restore pretrain weight
        :param pretrain_model_dir:
        :param is_pretrain:
        :return:
        """

        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))

        if checkpoint_path != None:
            restorer = tf.train.Saver()
            restorer.restore(sess, checkpoint_path)
            print("model restore from {0}".format(checkpoint_path))
        else:
            checkpoint_path = os.path.join(cfgs.PRETRAINED_CKPT, 'vgg16.ckpt')
            ckpt_scope = ['ssd_300_vgg/conv1', 'ssd_300_vgg_conv2', 'ssd_300_vgg/conv3', 'ssd_300_vgg_conv4',
                               'ssd_300_vgg/conv5']
            var_restore_dict = {}
            for scope in ckpt_scope:
                for var in slim.get_model_variables(scope=scope):
                    var_name = var.op.name
                    var_restore_dict[var_name] = var

            var_base_restore_dict = {var_name.replace(cfgs.MODEL_NAME, cfgs.BASE_NETWORK_NAME): var
                              for var_name, var in var_restore_dict.items()}

            restore_variables = var_restore_dict
            for key, item in restore_variables.items():
                print("var_in_graph: ", item.name)
                print("var_in_ckpt: ", key)

            for var_name, var_shape in tf.train.list_variables(checkpoint_path):

                if var_name in var_base_restore_dict.keys():
                    # get_variable
                    var_value = tf.train.load_variable(checkpoint_path, var_name)

                    # dst_var = dst_var_map[var_name_map[var_name]]
                    # # ensure the checkpoint variable shape same as  graph variable
                    # var_value = tf.reshape(var_value, dst_var.shape)
                    # assign value to variable of graph
                    tf.assign(var_base_restore_dict[var_name], value=var_value)
                else:
                    pass

            print("model restore from {0}".format(checkpoint_path))
            print("restore from pretrained_weighs in IMAGE_NET")

        return checkpoint_path


# =========================================================================== #
# SSD tools...
# =========================================================================== #
def ssd_size_bounds_to_values(size_bounds,
                              n_feat_layers,
                              img_shape=(300, 300)):
    """Compute the reference sizes of the anchor boxes from relative bounds.
    The absolute values are measured in pixels, based on the network
    default size (300 pixels).

    This function follows the computation performed in the original
    implementation of SSD in Caffe.

    Return:
      list of list containing the absolute sizes at each scale. For each scale,
      the ratios only apply to the first value.
    """
    assert img_shape[0] == img_shape[1]

    img_size = img_shape[0]
    min_ratio = int(size_bounds[0] * 100)
    max_ratio = int(size_bounds[1] * 100)
    step = int(math.floor((max_ratio - min_ratio) / (n_feat_layers - 2)))
    # Start with the following smallest sizes.
    sizes = [[img_size * size_bounds[0] / 2, img_size * size_bounds[0]]]
    for ratio in range(min_ratio, max_ratio + 1, step):
        sizes.append((img_size * ratio / 100.,
                      img_size * (ratio + step) / 100.))
    return sizes


def ssd_feat_shapes_from_net(predictions, default_shapes=None):
    """Try to obtain the feature shapes from the prediction layers. The latter
    can be either a Tensor or Numpy ndarray.

    Return:
      list of feature shapes. Default values if predictions shape not fully
      determined.
    """
    feat_shapes = []
    for l in predictions:
        # Get the shape, from either a np array or a tensor.
        if isinstance(l, np.ndarray):
            shape = l.shape
        else:
            shape = l.get_shape().as_list()
        shape = shape[1:4]
        # Problem: undetermined shape...
        if None in shape:
            return default_shapes
        else:
            feat_shapes.append(shape)
    return feat_shapes



# =========================================================================== #
# Functional definition of VGG-based SSD 300.
# =========================================================================== #
def tensor_shape(x, rank=3):
    """Returns the dimensions of a tensor.
    Args:
      image: A N-D Tensor of shape.
    Returns:
      A list of dimensions. Dimensions that are statically known are python
        integers,otherwise they are integer scalar tensors.
    """
    if x.get_shape().is_fully_defined():
        return x.get_shape().as_list()
    else:
        static_shape = x.get_shape().with_rank(rank).as_list()
        dynamic_shape = tf.unstack(tf.shape(x), rank)
        return [s if s is not None else d
                for s, d in zip(static_shape, dynamic_shape)]


def ssd_multibox_layer(inputs,
                       num_classes,
                       sizes,
                       ratios=[1],
                       normalization=-1,
                       bn_normalization=False):
    """Construct a multibox layer, return a class and localization predictions.
    """
    net = inputs
    if normalization > 0:
        net = custom_layers.l2_normalization(net, scaling=True)
    # Number of anchors.
    num_anchors = len(sizes) + len(ratios)

    # Location.
    num_loc_pred = num_anchors * 4
    loc_pred = slim.conv2d(net, num_loc_pred, [3, 3], activation_fn=None,
                           scope='conv_loc')
    loc_pred = custom_layers.channel_to_last(loc_pred)
    loc_pred = tf.reshape(loc_pred,
                          tensor_shape(loc_pred, 4)[:-1]+[num_anchors, 4])
    # Class prediction.
    num_cls_pred = num_anchors * num_classes
    cls_pred = slim.conv2d(net, num_cls_pred, [3, 3], activation_fn=None,
                           scope='conv_cls')
    cls_pred = custom_layers.channel_to_last(cls_pred)
    cls_pred = tf.reshape(cls_pred,
                          tensor_shape(cls_pred, 4)[:-1]+[num_anchors, num_classes])
    return cls_pred, loc_pred


def ssd_net(inputs,
            num_classes=SSDNet.default_params.num_classes,
            feat_layers=SSDNet.default_params.feat_layers,
            anchor_sizes=SSDNet.default_params.anchor_sizes,
            anchor_ratios=SSDNet.default_params.anchor_ratios,
            normalizations=SSDNet.default_params.normalizations,
            is_training=True,
            dropout_keep_prob=0.5,
            prediction_fn=slim.softmax,
            reuse=None,
            scope='ssd_300_vgg'):
    """SSD net definition.
    """
    # if data_format == 'NCHW':
    #     inputs = tf.transpose(inputs, perm=(0, 3, 1, 2))

    # End_points collect relevant activations for external use.
    end_points = {}
    with tf.variable_scope(scope, 'ssd_300_vgg', [inputs], reuse=reuse):
        # Original VGG-16 blocks.
        net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')
        end_points['block1'] = net
        net = slim.max_pool2d(net, [2, 2], scope='pool1')
        # Block 2.
        net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')
        end_points['block2'] = net
        net = slim.max_pool2d(net, [2, 2], scope='pool2')
        # Block 3.
        net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')
        end_points['block3'] = net
        net = slim.max_pool2d(net, [2, 2], scope='pool3')
        # Block 4.
        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')
        end_points['block4'] = net
        net = slim.max_pool2d(net, [2, 2], scope='pool4')
        # Block 5.
        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')
        end_points['block5'] = net
        net = slim.max_pool2d(net, [3, 3], stride=1, scope='pool5')

        # Additional SSD blocks.
        # Block 6: let's dilate the hell out of it!
        net = slim.conv2d(net, 1024, [3, 3], rate=6, scope='conv6')
        end_points['block6'] = net
        net = tf.layers.dropout(net, rate=dropout_keep_prob, training=is_training)
        # Block 7: 1x1 conv. Because the fuck.
        net = slim.conv2d(net, 1024, [1, 1], scope='conv7')
        end_points['block7'] = net
        net = tf.layers.dropout(net, rate=dropout_keep_prob, training=is_training)

        # Block 8/9/10/11: 1x1 and 3x3 convolutions stride 2 (except lasts).
        end_point = 'block8'
        with tf.variable_scope(end_point):
            net = slim.conv2d(net, 256, [1, 1], scope='conv1x1')
            net = custom_layers.pad2d(net, pad=(1, 1))
            net = slim.conv2d(net, 512, [3, 3], stride=2, scope='conv3x3', padding='VALID')
        end_points[end_point] = net
        end_point = 'block9'
        with tf.variable_scope(end_point):
            net = slim.conv2d(net, 128, [1, 1], scope='conv1x1')
            net = custom_layers.pad2d(net, pad=(1, 1))
            net = slim.conv2d(net, 256, [3, 3], stride=2, scope='conv3x3', padding='VALID')
        end_points[end_point] = net
        end_point = 'block10'
        with tf.variable_scope(end_point):
            net = slim.conv2d(net, 128, [1, 1], scope='conv1x1')
            net = slim.conv2d(net, 256, [3, 3], scope='conv3x3', padding='VALID')
        end_points[end_point] = net
        end_point = 'block11'
        with tf.variable_scope(end_point):
            net = slim.conv2d(net, 128, [1, 1], scope='conv1x1')
            net = slim.conv2d(net, 256, [3, 3], scope='conv3x3', padding='VALID')
        end_points[end_point] = net

        # Prediction and localisations layers.
        predictions = []
        logits = []
        localisations = []
        for i, layer in enumerate(feat_layers):
            with tf.variable_scope(layer + '_box'):
                p, l = ssd_multibox_layer(end_points[layer],
                                          num_classes,
                                          anchor_sizes[i],
                                          anchor_ratios[i],
                                          normalizations[i])
            predictions.append(prediction_fn(p))
            logits.append(p)
            localisations.append(l)

        return predictions, localisations, logits, end_points
ssd_net.default_image_size = 300


def ssd_arg_scope(weight_decay=0.0005, data_format='NHWC'):
    """Defines the VGG arg scope.

    Args:
      weight_decay: The l2 regularization coefficient.

    Returns:
      An arg_scope.
    """
    with slim.arg_scope([slim.conv2d, slim.fully_connected],
                        activation_fn=tf.nn.relu,
                        weights_regularizer=slim.l2_regularizer(weight_decay),
                        weights_initializer=tf.contrib.layers.xavier_initializer(),
                        biases_initializer=tf.zeros_initializer()):
        with slim.arg_scope([slim.conv2d, slim.max_pool2d],
                            padding='SAME',
                            data_format=data_format):
            with slim.arg_scope([custom_layers.pad2d,
                                 custom_layers.l2_normalization,
                                 custom_layers.channel_to_last],
                                data_format=data_format) as sc:
                return sc

# =========================================================================== #
# Caffe scope: importing weights at initialization.
# =========================================================================== #
def ssd_arg_scope_caffe(caffe_scope):
    """Caffe scope definition.

    Args:
      caffe_scope: Caffe scope object with loaded weights.

    Returns:
      An arg_scope.
    """
    # Default network arg scope.
    with slim.arg_scope([slim.conv2d],
                        activation_fn=tf.nn.relu,
                        weights_initializer=caffe_scope.conv_weights_init(),
                        biases_initializer=caffe_scope.conv_biases_init()):
        with slim.arg_scope([slim.fully_connected],
                            activation_fn=tf.nn.relu):
            with slim.arg_scope([custom_layers.l2_normalization],
                                scale_initializer=caffe_scope.l2_norm_scale_init()):
                with slim.arg_scope([slim.conv2d, slim.max_pool2d],
                                    padding='SAME') as sc:
                    return sc


